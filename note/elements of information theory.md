[toc]

# 信息论基础_学习笔记

## 绪论

无，等学完

## 熵、相对熵和互信息

### 开篇

1. 熵是一个随机变量的自信息量
2. 互信息量是一种测度，表现为**度量**一个随机变量**包含另一个随机变量的信息量**

3. 相对熵刻画了**两个概率分布之间距离**的度量，互信息是相对熵的**特殊情况**

## 熵

1. 概念：熵表示的是对随机变量**不确定度**的度量。
2. 规定$p(x)$和$p(y)$分别代表两种不同的概率密度函数，也就是说描述的是不同两个随机变量，换一句话说，$p_X(x) = {p(x),x\in\chi}$，而$p_Y(y) = p(y), y\in \gamma$。
3. 定义：一个离散型随机变量$X$的熵$H(X)$定义为：$H(X) = - \sum_{x\in \chi}p(x)\cdot\log_2 p(x)$，其中$0\log_20 = 0$，因为加上零概率的项不改变熵的值。
4. 若用$E$表示数学期望。如果$X\sim p(x)$，则随机变量$g(X)$的期望值是：$E_pg(X) = \sum_{x\in\chi}g(x)p(x)$，简称$Eg(X)$。**特别关注**，当$g(X) = log_2 \frac{1}{p(X)}$，这里，$g(X)$关于$p(x)$，而$g(X)$本身又是$p(x)$的映射，这种怪异的行为称为自我指涉。
5. $H(x) = E_p \cdot g(X) = E_p \cdot \log_2\frac{1}{p(X)} = E_{p(X)}\log_2p(X)$

6. 引理2.1.1：$H(X) \geq 0$
7. 引理2.1.2：$H_b(x) = \log_ba\cdot H_a(x)$，换底公式$\log_a b = {\log_cb \over \log_c a}$，换句话说也就是$\log_b p = \log_ap \cdot \log_b a$。这意味着要改变熵的对数，只要乘上以恰当的换底因子，熵就可以从一个底换成另一个底了。

## 联合熵和条件熵

先前的熵是用于单个随机变量的熵。现在将条件推广到两个随机变量的情形。但是如果将两个随机变量看成单个向量型的随机变量，比如$X$、$Y$看成$(X, Y)$。定义也大致类似。

1. 联合熵的定义：对于服从联合分布为$p(x, y)$的一对离散随机变量$(X, Y)$，其联合熵$H(X,Y) = - \sum_{x\in\chi}\sum_{y\in\gamma}p(x, y)\log_2p(x, y) = -E_{p(X,Y)}\log_2{p(X, Y)}$
2. 条件熵的定义：若$(X,Y)\sim p(x, y)$，条件熵$H(Y|X)$定义为：

$$
H(Y|X) = \sum_{x\in\chi}p(x)\cdot H(Y|X=x)
$$

$$
H(Y|X = x) = -\sum_{y\in\gamma}p(y|X=x)\cdot \log_2p(y|X=x)
$$

$$
H(Y|X)=-\sum_{x\in\chi}\sum_{y\in\gamma}p(x)\cdot p(y|x)\cdot \log_2p(y|x)=-\sum_{x\in\chi}\sum_{y\in\gamma}p(x, y)\cdot \log_2p(y|x)
$$

$$
H(Y|X) = -E_{p(X,Y)}\log_2p(Y|X)
$$

3. 定理链式法则：条件熵和联合熵以及熵具有整合意义

$$
H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X|Y)
$$

等价为，两边取$-\sum_{x\in chi}\sum_{y\in\gamma}p(x, y)$数学期望，与上面等价：
$$
\log_2p(X,Y) = \log_2p(X) + \log_2p(Y|X) = \log_2[p(X)\cdot p(Y|X)]
$$

## 相对熵和互信息

1. 相对熵描述对于一个随机变量产生的两个不同随机分布之间距离的度量。
2. 相对熵又称KL散度。
3. 相对熵的定义：设对于一个随机变量而言，存在着两种不同的随机分布分别为$p(x)$和$q(x)$，相对熵或者KL距离定义为：

$$
D(p||q) = \sum_{x\in\chi}p(x)\cdot\log_2{p(x)\over q(x)} = E_{p(x)}\log_2{p(x)\over q(x)}
$$

4. 互信息描述了一个随机变量包含另一个随机变量的度量。
5. 互信息也描述了在给定另一随机变量知识的条件下，对原随机变量不确定度的缩减量。
6. 互信息的定义：考虑两个随机变量$X$和$Y$，他们的联合概率密度函数为$p(x, y)$，其边际概率密度函数分别为$p(x)$和$p(y)$。互信息$I(X;Y)$为联合分布$p(x,y)$和边际分布乘积$p(x)\cdot p(y)$之间的**相对熵**。定义：

$$
I(X;Y) = \sum_{x\in\chi}\sum_{y\in\gamma}p(x, y)\log_2{p(x,y)\over p(x)\cdot p(y)} = E_{p(x,y)}\log_2{p(x, y)\over p(x)\cdot p(y)}
$$

