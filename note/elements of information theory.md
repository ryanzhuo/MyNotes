[toc]

# 信息论基础_学习笔记

## 绪论

无，等学完

## 熵、相对熵和互信息

### 开篇

1. 熵是一个随机变量的自信息量
2. 互信息量是一种测度，表现为**度量**一个随机变量**包含另一个随机变量的信息量**

3. 相对熵刻画了**两个概率分布之间距离**的度量，互信息是相对熵的**特殊情况**

## 熵

1. 概念：熵表示的是对随机变量**不确定度**的度量。
2. 规定$p(x)$和$p(y)$分别代表两种不同的概率密度函数，也就是说描述的是不同两个随机变量，换一句话说，$p_X(x) = {p(x),x\in\chi}$，而$p_Y(y) = p(y), y\in \gamma$。
3. 定义：一个离散型随机变量$X$的熵$H(X)$定义为：$H(X) = - \sum_{x\in \chi}p(x)\cdot\log_2 p(x)$，其中$0\log_20 = 0$，因为加上零概率的项不改变熵的值。
4. 若用$E$表示数学期望。如果$X\sim p(x)$，则随机变量$g(X)$的期望值是：$E_pg(X) = \sum_{x\in\chi}g(x)p(x)$，简称$Eg(X)$。**特别关注**，当$g(X) = log_2 \frac{1}{p(X)}$，这里，$g(X)$关于$p(x)$，而$g(X)$本身又是$p(x)$的映射，这种怪异的行为称为自我指涉。
5. $H(x) = E_p \cdot g(X) = E_p \cdot \log_2\frac{1}{p(X)}$

6. 引理2.1.1：$H(X) \geq 0$
7. 引理2.1.2：$H_b(x) = \log_ba\cdot H_a(x)$，换底公式$\log_a b = {\log_cb \over \log_c a}$，换句话说也就是$\log_b p = \log_ap \cdot \log_b a$。这意味着要改变熵的对数，只要乘上以恰当的换底因子，熵就可以从一个底换成另一个底了。

## 联合熵和条件熵

先前的熵是用于单个随机变量的熵。现在将条件推广到两个随机变量的情形。但是如果将两个随机变量看成单个向量型的随机变量，比如$X$、$Y$看成$(X, Y)$。定义也大致类似。

1. 定义：对于服从联合分布为$p(x, y)$的一对离散随机变量$(X, Y)$，其联合熵$H(X,Y)$