[toc]

# 信息论基础_学习笔记

## 绪论

无，等学完

## 熵、相对熵和互信息

### 开篇

1. 熵是一个随机变量的自信息量
2. 互信息量是一种测度，表现为**度量**一个随机变量**包含另一个随机变量的信息量**

3. 相对熵刻画了**两个概率分布之间距离**的度量，互信息是相对熵的**特殊情况**

## 熵

1. 概念：熵表示的是对随机变量**不确定度**的度量
2. 规定$p(x)$和$p(y)$分别代表两种不同的概率密度函数，也就是说描述的是不同两个随机变量，换一句话说，$p_X(x) = {p(x),x\in}$